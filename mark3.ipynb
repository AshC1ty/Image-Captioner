{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, RepeatVector, Dense, LSTM\n",
    "from tensorflow.keras.layers import Embedding, Dropout, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.merge import add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get all filenames of the images\n",
    "folder = \"../input/flickr8k/Flickr_Data/Flickr_Data/Images/\"\n",
    "images = os.listdir(folder)\n",
    "\n",
    "# Load the CNN Architecture with Imagenet as weights\n",
    "image_model = ResNet50(weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "model_new = tf.keras.Model(image_model.input,image_model.layers[-2].output)\n",
    "\n",
    "# Store image features in dictionary\n",
    "img_features = dict()  \n",
    "for img in images: \n",
    "    img1 = image.load_img(folder + img, target_size=(224, 224))\n",
    "    x = image.img_to_array(img1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    fea_x = model_new.predict(x)\n",
    "    fea_x1 = np.reshape(fea_x , fea_x.shape[1])\n",
    "    img_features[img] = fea_x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get All Captions\n",
    "fn = \"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\n",
    "f = open(fn, 'r')\n",
    "capts = f.read()\n",
    "#Group all captions by filename, for references\n",
    "captions = dict()\n",
    "i = 0\n",
    "\n",
    "try:\n",
    "    for line in capts.split(\"\\n\"):\n",
    "        txt = line.split('\\t')\n",
    "        fn = txt[0].split('#')[0]\n",
    "        if fn not in captions.keys():\n",
    "            captions[fn] = [txt[1]]\n",
    "        else:\n",
    "            captions[fn].append(txt[1])\n",
    "        i += 1\n",
    "except:\n",
    "    pass #pass Model\n",
    "    \n",
    "\n",
    "def getCaptions(path):\n",
    "    \n",
    "    f = open(path, 'r')\n",
    "    capts = f.read()\n",
    "    desc = dict()\n",
    "\n",
    "    try:\n",
    "        for line in capts.split(\"\\n\"):\n",
    "            image_id = line\n",
    "            image_descs = captions[image_id]\n",
    "\n",
    "            for des in image_descs:\n",
    "                ws = des.split(\" \")\n",
    "                w = [word for word in ws if word.isalpha()]\n",
    "                des = \"startseq \" + \" \".join(w) + \" endseq\"\n",
    "                if image_id not in desc:\n",
    "                    desc[image_id] = list()\n",
    "                desc[image_id].append(des)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return desc\n",
    "\n",
    "# Split captions\n",
    "train_caps = getCaptions(\"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt\")\n",
    "val_caps = getCaptions(\"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preparing to make word-index and index-word\n",
    "# (adding all training captions to a list)\n",
    "train_captions = []\n",
    "for key, desc_list in train_caps.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        train_captions.append(desc_list[i])\n",
    "\n",
    "# Tokenize top 5000 words in Train Captions\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size,\n",
    "                      oov_token=\"<unk>\",\n",
    "                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images + Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_fns = list(train_caps.keys())\n",
    "train_set = dict((k, img_features[k]) for k in train_fns)\n",
    "val_fns = list(val_caps.keys())\n",
    "val_set = dict((k, img_features[k]) for k in val_fns)\n",
    "fn_test = \"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt\"\n",
    "f = open(fn_test, 'r')\n",
    "t = f.read()\n",
    "\n",
    "test_fns= t.split(\"\\n\")\n",
    "test_set = dict((k, img_features[k]) for k in list(test_fns[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Training & Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test the data generator\n",
    "generator = data_generator(train_caps, train_set, tokenizer, max_length, vocab_size)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load Glove vectors\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(\"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\", encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.models.Sequential()\n",
    "\n",
    "image_model.add(Dense(embedding_dim, input_shape=(2048,), activation='relu'))\n",
    "image_model.add(RepeatVector(max_length))\n",
    "\n",
    "language_model = tf.keras.models.Sequential()\n",
    "\n",
    "language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "language_model.add(LSTM(256, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(embedding_dim)))\n",
    "\n",
    "conca = Concatenate()([image_model.output, language_model.output])\n",
    "x = LSTM(128, return_sequences=True)(conca)\n",
    "x = LSTM(512, return_sequences=False)(x)\n",
    "x = Dense(vocab_size)(x)\n",
    "out = Activation('softmax')(x)\n",
    "model_1 = Model(inputs=[image_model.input, language_model.input], outputs = out)\n",
    "\n",
    "model_1.layers[2].set_weights([embedding_matrix])\n",
    "model_1.layers[2].trainable = False\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate = 0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 50\n",
    "steps = len(train_caps)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_caps, train_set, tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "    model_1.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    return tokenizer.index_word.get(integer)\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #print(\"sequence after tok: \", sequence)\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        if i==0:\n",
    "            photo = np.expand_dims(photo, axis=0)\n",
    "        #print(\"photo: \", photo)\n",
    "        #print(\"sequence: \", sequence)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test the predict function\n",
    "tmpimg1 = np.expand_dims(np.array(photo), axis=0)\n",
    "print(tmpimg1.shape)\n",
    "tmpcap1 = pad_sequences([[3]], maxlen=max_length)\n",
    "print(tmpcap1.shape)\n",
    "tmpout1 = model_1.predict([tmpimg1, tmpcap1], verbose=0)\n",
    "print(tmpout1.shape)\n",
    "print(tmpout1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def image_to_feat_vec(imagePath):\n",
    "    img1 = image.load_img(imagePath, target_size=(224, 224))\n",
    "    x = image.img_to_array(img1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    fea_x = model_new.predict(x)\n",
    "    fea_x1 = np.reshape(fea_x , fea_x.shape[1])\n",
    "    return fea_x1\n",
    "\n",
    "imagePath = \"../input/garage-detection-unofficial-ssl-challenge/GarageImages/GarageImages/image1086.jpg\"\n",
    "photo = image_to_feat_vec(imagePath)\n",
    "print(\"Predicted Caption:\", generate_desc(model_1, tokenizer, photo, max_length))\n",
    "Image.open(imagePath)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 145129,
     "sourceId": 343604,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 464180,
     "sourceId": 872462,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 723240,
     "sourceId": 1259449,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30085,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
